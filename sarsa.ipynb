{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function ε_greedy_action(Q, s, actions, ε)\n",
    "    if rand() < ε\n",
    "        return rand(actions)\n",
    "    else\n",
    "        return argmax(a -> Q[s][a], actions)\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function sarsa_learn(mdp, α, ε, γ=0.99; episodes=10_000, max_steps=100)\n",
    "    sta = collect(states(mdp))\n",
    "    ac = collect(actions(mdp))\n",
    "    function sarsa_learn(mdp, α, ε, γ=0.99; episodes=10_000, max_steps=100)\n",
    "        sta = collect(states(mdp))\n",
    "        ac = collect(actions(mdp))\n",
    "    \n",
    "        Q = Dict(s => Dict(a => 0.0 for a in ac) for s in sta)\n",
    "        rng = MersenneTwister(42)\n",
    "    \n",
    "        for ep in 1:episodes\n",
    "            s = rand(rng, sta)\n",
    "            a = ε_greedy_action(Q, s, ac, ε)\n",
    "    \n",
    "            for step in 1:max_steps\n",
    "                transitions = transition(mdp, s, a)\n",
    "                probs = [p for (_, p) in transitions]\n",
    "                next_states = [sp for (sp, _) in transitions]\n",
    "                s′ = sample(rng, next_states, Weights(probs))\n",
    "    \n",
    "                r = reward(mdp, s, a, s′)\n",
    "                a′ = ε_greedy_action(Q, s′, ac, ε)\n",
    "    \n",
    "                Q[s][a] += α * (r + γ * Q[s′][a′] - Q[s][a])\n",
    "                s, a = s′, a′\n",
    "            end\n",
    "        end\n",
    "    \n",
    "        policy = Dict(s => argmax(a -> Q[s][a], ac) for s in sta)\n",
    "        return policy, Q\n",
    "    end\n",
    "    \n",
    "    Q = Dict(s => Dict(a => 0.0 for a in ac) for s in sta)\n",
    "    rng = MersenneTwister(42)\n",
    "\n",
    "    for ep in 1:episodes\n",
    "        s = rand(rng, sta)\n",
    "        a = ε_greedy_action(Q, s, ac, ε)\n",
    "\n",
    "        for step in 1:max_steps\n",
    "            sp, r, _ = gen(mdp, s, a, rng)  # ← NEW: uses gen instead of transition + reward\n",
    "\n",
    "            # Choose next action a′\n",
    "            a′ = ε_greedy_action(Q, sp, ac, ε)\n",
    "\n",
    "            # SARSA update\n",
    "            Q[s][a] += α * (r + γ * Q[sp][a′] - Q[s][a])\n",
    "\n",
    "            # Move to next state-action pair\n",
    "            s, a = sp, a′\n",
    "        end\n",
    "    end\n",
    "\n",
    "    policy = Dict(s => argmax(a -> Q[s][a], ac) for s in sta)\n",
    "    return policy, Q\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy, Q = sarsa_learn(mdp, 0.1, 0.2)\n",
    "\n",
    "println(\"SARSA Policy:\")\n",
    "for st in sort(collect(keys(policy)))\n",
    "    println(\"Inventory $st → Action: \", policy[st] == 1 ? \"Order\" : \"Do Not Order\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function evaluate_policy(mdp, policy; episodes=100, max_steps=100)\n",
    "    total_reward = 0.0\n",
    "    rng = MersenneTwister(123)\n",
    "\n",
    "    for _ in 1:episodes\n",
    "        s = 10  # initial inventory level (as per problem)\n",
    "        r_total = 0.0\n",
    "\n",
    "        for _ in 1:max_steps\n",
    "            a = policy[s]\n",
    "            s′, r, _ = gen(mdp, s, a, rng)  # ← use gen() instead of transition + reward\n",
    "            r_total += r\n",
    "            s = s′\n",
    "        end\n",
    "\n",
    "        total_reward += r_total\n",
    "    end\n",
    "\n",
    "    return total_reward / episodes\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = [0.2, 0.1, 0.01, 1e-5]\n",
    "epsilons = [0.2, 0.1, 0.01, 1e-5]\n",
    "\n",
    "results = []\n",
    "\n",
    "for α in alphas, ε in epsilons\n",
    "    println(\"Training SARSA with α = $α, ε = $ε\")\n",
    "    policy, Q = sarsa_learn(mdp, α, ε)\n",
    "    avg_reward = evaluate_policy(mdp, policy)\n",
    "    push!(results, (α, ε, round(avg_reward, digits=2)))\n",
    "end\n",
    "\n",
    "println(\"\\nPerformance Summary (Average Reward per Policy):\")\n",
    "println(\"α\\tϵ\\tAverage Reward\")\n",
    "for (α, ε, reward) in results\n",
    "    println(\"$α\\t$ε\\t$reward\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = DataFrame(results, [:alpha, :epsilon, :avg_reward])\n",
    "alphas = sort(unique(df.alpha))\n",
    "epsilons = sort(unique(df.epsilon))\n",
    "\n",
    "# Build Z matrix safely\n",
    "Z = fill(NaN, length(alphas), length(epsilons))\n",
    "for (i, α) in enumerate(alphas)\n",
    "    for (j, ϵ) in enumerate(epsilons)\n",
    "        row = filter(r -> r.alpha == α && r.epsilon == ϵ, eachrow(df))\n",
    "        if !isempty(row)\n",
    "            Z[i, j] = row[1][:avg_reward]\n",
    "        end\n",
    "    end\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap(\n",
    "    epsilons,\n",
    "    alphas,\n",
    "    Z;\n",
    "    xlabel = \"ε (Exploration Rate)\",\n",
    "    ylabel = \"α (Learning Rate)\",\n",
    "    title = \"SARSA Performance (Average Reward)\",\n",
    "    colorbar_title = \"Reward\",\n",
    "    c = :coolwarm,\n",
    "    clims = (minimum(Z), maximum(Z))  # adjust color scaling\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The value iteration algorithm to find the optimal policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function value_iteration_gen(mdp; γ=POMDPs.discount(mdp), θ=1e-3, max_iter=100000, n_samples=100)\n",
    "    states = collect(POMDPs.states(mdp))\n",
    "    actions_per_state = Dict(s => collect(POMDPs.actions(mdp, s)) for s in states)\n",
    "\n",
    "    V = Dict(s => 0.0 for s in states)\n",
    "    π = Dict(s => first(actions_per_state[s]) for s in states)\n",
    "\n",
    "    rng = Random.MersenneTwister(42)  # fixe pour reproductibilité\n",
    "\n",
    "    for iter in 1:max_iter\n",
    "        Δ = 0.0\n",
    "        V_new = copy(V)\n",
    "\n",
    "        for s in states\n",
    "            v_old = V[s]\n",
    "            best_value = -Inf\n",
    "            best_action = nothing\n",
    "\n",
    "            for a in actions_per_state[s]\n",
    "                total = 0.0\n",
    "                for _ in 1:n_samples\n",
    "                    sp, r, _ = POMDPs.gen(mdp, s, a, rng)\n",
    "                    total += r + γ * V[sp]\n",
    "                end\n",
    "                value = total / n_samples\n",
    "\n",
    "                if value > best_value\n",
    "                    best_value = value\n",
    "                    best_action = a\n",
    "                end\n",
    "            end\n",
    "\n",
    "            V_new[s] = best_value\n",
    "            π[s] = best_action\n",
    "            Δ = max(Δ, abs(v_old - best_value))\n",
    "        end\n",
    "\n",
    "        V = V_new\n",
    "        if Δ < θ\n",
    "            println(\"Convergence atteinte (Δ < θ = $θ).\")\n",
    "            break\n",
    "        end\n",
    "    end\n",
    "\n",
    "    return V, π\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V_opt, π_opt = value_iteration_gen(mdp)\n",
    "for s in sort(collect(keys(π_opt)))\n",
    "    println(\"Stock $s → Action optimale : \", π_opt[s] == 1 ? \"Commander\" : \"Ne rien faire\")\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The linear programming formulation to find the optimal policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "state_space = 0:MAX_INVENTORY\n",
    "action_space = [0, 1]\n",
    "γ = discount(mdp)\n",
    "\n",
    "\n",
    "function transition_outcomes(s, a)\n",
    "    outcomes = []\n",
    "    order_qty = a == 1 ? min(ORDER_SIZE, MAX_INVENTORY - s) : 0\n",
    "    new_stock = s + order_qty\n",
    "\n",
    "    for d in support(demand_dist)\n",
    "        sold = min(d, new_stock)\n",
    "        sp = new_stock - sold\n",
    "        prob = pdf(demand_dist, d)\n",
    "        push!(outcomes, (sp, prob, d))\n",
    "    end\n",
    "    return outcomes\n",
    "end\n",
    "\n",
    "\n",
    "function immediate_reward(s, a, sp, d)\n",
    "    order_qty = a == 1 ? min(ORDER_SIZE, MAX_INVENTORY - s) : 0\n",
    "    new_stock = s + order_qty\n",
    "\n",
    "    lost_sales = max(d - new_stock, 0)\n",
    "    in_store = min(sp, MAX_STORE)\n",
    "    in_parking = max(sp - MAX_STORE, 0)\n",
    "\n",
    "    cost = 0\n",
    "    cost += a == 1 ? order_cost : 0\n",
    "    cost += in_store * holding_cost_store\n",
    "    cost += in_parking * holding_cost_parking\n",
    "    cost += lost_sales * stockout_penalty\n",
    "\n",
    "    return -cost\n",
    "end\n",
    "\n",
    "\n",
    "model = Model(GLPK.Optimizer)\n",
    "@variable(model, v[s in state_space])\n",
    "\n",
    "for s in state_space\n",
    "    for a in action_space\n",
    "        expected_value = 0.0\n",
    "        for (sp, prob, d) in transition_outcomes(s, a)\n",
    "            r = immediate_reward(s, a, sp, d)\n",
    "            expected_value += prob * (r + γ * v[sp])\n",
    "        end\n",
    "        @constraint(model, v[s] ≥ expected_value)\n",
    "    end\n",
    "end\n",
    "\n",
    "@objective(model, Min, sum(v[s] for s in state_space))\n",
    "optimize!(model)\n",
    "\n",
    "\n",
    "V_lp = Dict(s => JuMP.value(v[s]) for s in state_space)\n",
    "\n",
    "\n",
    "π_lp = Dict()\n",
    "\n",
    "for s in state_space\n",
    "    best_value = -Inf\n",
    "    best_action = nothing\n",
    "    for a in action_space\n",
    "        total = 0.0\n",
    "        for (sp, prob, d) in transition_outcomes(s, a)\n",
    "            r = immediate_reward(s, a, sp, d)\n",
    "            total += prob * (r + γ * V_lp[sp])\n",
    "        end\n",
    "        if total > best_value\n",
    "            best_value = total\n",
    "            best_action = a\n",
    "        end\n",
    "    end\n",
    "    π_lp[s] = best_action\n",
    "end\n",
    "\n",
    "\n",
    "println(\"📊 Politique optimale (via PL) :\")\n",
    "for s in state_space\n",
    "    println(\"Stock $s → Action optimale : \", π_lp[s] == 1 ? \"Commander\" : \"Ne rien faire\")\n",
    "end\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.3",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
