{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description du MDP et exemple d'utilisation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QuickMDP{Base.UUID(\"16aa6965-baab-40b9-927d-f210292ddded\"), Int64, Int64, @NamedTuple{stateindex::Dict{Int64, Int64}, isterminal::Bool, actionindex::Dict{Int64, Int64}, initialstate::Deterministic{Int64}, states::UnitRange{Int64}, actions::Vector{Int64}, discount::Float64, gen::var\"#11#12\"}}((stateindex = Dict(5 => 6, 16 => 17, 7 => 8, 20 => 21, 12 => 13, 8 => 9, 17 => 18, 1 => 2, 19 => 20, 0 => 1…), isterminal = false, actionindex = Dict(0 => 1, 1 => 2), initialstate = Deterministic{Int64}(10), states = 0:20, actions = [0, 1], discount = 0.99, gen = var\"#11#12\"()))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "using POMDPs\n",
    "using QuickPOMDPs\n",
    "using Distributions\n",
    "using Random\n",
    "using POMDPTools: Deterministic\n",
    "\n",
    "const MAX_INVENTORY = 20\n",
    "const MAX_STORE = 10\n",
    "const ORDER_SIZE = 5\n",
    "\n",
    "const holding_cost_store = 2\n",
    "const holding_cost_parking = 4\n",
    "const order_cost = 20\n",
    "const stockout_penalty = 50\n",
    "\n",
    "const demand_dist = DiscreteUniform(0, 10)\n",
    "\n",
    "mdp = QuickMDP(\n",
    "    \n",
    "    states = 0:MAX_INVENTORY,\n",
    "    actions = [0, 1],\n",
    "    discount = 0.99,\n",
    "\n",
    "    gen = function (s, a, rng)\n",
    "        order_qty = a == 1 ? min(ORDER_SIZE, MAX_INVENTORY - s) : 0\n",
    "        new_stock = s + order_qty\n",
    "\n",
    "        d = rand(rng, demand_dist)\n",
    "        sold = min(d, new_stock)\n",
    "        sp = new_stock - sold\n",
    "\n",
    "        lost_sales = max(d - new_stock, 0)\n",
    "\n",
    "        in_store = min(sp, MAX_STORE)\n",
    "        in_parking = max(sp - MAX_STORE, 0)\n",
    "\n",
    "        cost = 0\n",
    "        cost += a == 1 ? order_cost : 0\n",
    "        cost += in_store * holding_cost_store\n",
    "        cost += in_parking * holding_cost_parking\n",
    "        cost += lost_sales * stockout_penalty\n",
    "\n",
    "        r = -cost\n",
    "\n",
    "        return (sp, r, d)\n",
    "    end,\n",
    "    initialstate = Deterministic(10)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From 7 --(a=0)--> 5,  Demand: 2, Reward = -10\n",
      "From 7 --(a=0)--> 0,  Demand: 9, Reward = -100\n",
      "From 7 --(a=0)--> 3,  Demand: 4, Reward = -6\n",
      "From 7 --(a=0)--> 0,  Demand: 7, Reward = 0\n",
      "From 7 --(a=0)--> 0,  Demand: 7, Reward = 0\n",
      "From 7 --(a=0)--> 0,  Demand: 7, Reward = 0\n",
      "From 7 --(a=0)--> 0,  Demand: 10, Reward = -150\n",
      "From 7 --(a=0)--> 0,  Demand: 8, Reward = -50\n",
      "From 7 --(a=0)--> 5,  Demand: 2, Reward = -10\n",
      "From 7 --(a=0)--> 6,  Demand: 1, Reward = -12\n"
     ]
    }
   ],
   "source": [
    "using Random\n",
    "rng = MersenneTwister(42)\n",
    "s = 7\n",
    "a = 0\n",
    "for i in 1:10\n",
    "    sp, r, d = POMDPs.gen(mdp, s, a, rng)\n",
    "    println(\"From $s --(a=$a)--> $sp,  Demand: $d, Reward = $r\")\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `C:\\Users\\mathj\\.julia\\environments\\v1.11\\Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `C:\\Users\\mathj\\.julia\\environments\\v1.11\\Manifest.toml`\n"
     ]
    }
   ],
   "source": [
    "import Pkg; Pkg.add(\"TabularTDLearning\")\n",
    "using POMDPs\n",
    "using QuickPOMDPs\n",
    "using POMDPTools\n",
    "using TabularTDLearning\n",
    "using Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train_q_learning_agent (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function train_q_learning_agent(mdp, alpha::Float64, epsilon::Float64; iterations=10_000)\n",
    "    ql = QLearningSolver(\n",
    "        max_iterations=iterations,\n",
    "        alpha=alpha,\n",
    "        epsilon=epsilon,\n",
    "        gamma=mdp.discount\n",
    "    )\n",
    "    policy = solve(ql, mdp)\n",
    "    return policy, ql.q\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "evaluate_policy (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function evaluate_policy(mdp, policy; num_trials=100, max_steps=100)\n",
    "    sim = RolloutSimulator(max_steps=max_steps)\n",
    "    rewards = [simulate(sim, mdp, policy) for _ in 1:num_trials]\n",
    "    return mean(rewards)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The value iteration algorithm to find the optimal policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "value_iteration_gen (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function value_iteration_gen(mdp; γ=POMDPs.discount(mdp), θ=1e-4, max_iter=10000, n_samples=100)\n",
    "    states = collect(POMDPs.states(mdp))\n",
    "    actions_per_state = Dict(s => collect(POMDPs.actions(mdp, s)) for s in states)\n",
    "\n",
    "    V = Dict(s => 0.0 for s in states)\n",
    "    π = Dict(s => first(actions_per_state[s]) for s in states)\n",
    "\n",
    "    rng = Random.MersenneTwister(42)  # fixe pour reproductibilité\n",
    "\n",
    "    for iter in 1:max_iter\n",
    "        Δ = 0.0\n",
    "        V_new = copy(V)\n",
    "\n",
    "        for s in states\n",
    "            v_old = V[s]\n",
    "            best_value = -Inf\n",
    "            best_action = nothing\n",
    "\n",
    "            for a in actions_per_state[s]\n",
    "                total = 0.0\n",
    "                for _ in 1:n_samples\n",
    "                    sp, r, _ = POMDPs.gen(mdp, s, a, rng)\n",
    "                    total += r + γ * V[sp]\n",
    "                end\n",
    "                value = total / n_samples\n",
    "\n",
    "                if value > best_value\n",
    "                    best_value = value\n",
    "                    best_action = a\n",
    "                end\n",
    "            end\n",
    "\n",
    "            V_new[s] = best_value\n",
    "            π[s] = best_action\n",
    "            Δ = max(Δ, abs(v_old - best_value))\n",
    "        end\n",
    "\n",
    "        V = V_new\n",
    "        if Δ < θ\n",
    "            println(\"Convergence atteinte (Δ < θ = $θ).\")\n",
    "            break\n",
    "        end\n",
    "    end\n",
    "\n",
    "    return V, π\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stock 0 → Action optimale : Commander\n",
      "Stock 1 → Action optimale : Commander\n",
      "Stock 2 → Action optimale : Commander\n",
      "Stock 3 → Action optimale : Commander\n",
      "Stock 4 → Action optimale : Commander\n",
      "Stock 5 → Action optimale : Commander\n",
      "Stock 6 → Action optimale : Commander\n",
      "Stock 7 → Action optimale : Commander\n",
      "Stock 8 → Action optimale : Commander\n",
      "Stock 9 → Action optimale : Commander\n",
      "Stock 10 → Action optimale : Commander\n",
      "Stock 11 → Action optimale : Commander\n",
      "Stock 12 → Action optimale : Commander\n",
      "Stock 13 → Action optimale : Ne rien faire\n",
      "Stock 14 → Action optimale : Ne rien faire\n",
      "Stock 15 → Action optimale : Ne rien faire\n",
      "Stock 16 → Action optimale : Ne rien faire\n",
      "Stock 17 → Action optimale : Ne rien faire\n",
      "Stock 18 → Action optimale : Ne rien faire\n",
      "Stock 19 → Action optimale : Ne rien faire\n",
      "Stock 20 → Action optimale : Ne rien faire\n"
     ]
    }
   ],
   "source": [
    "V_opt, π_opt = value_iteration_gen(mdp)\n",
    "for s in sort(collect(keys(π_opt)))\n",
    "    println(\"Stock $s → Action optimale : \", π_opt[s] == 1 ? \"Commander\" : \"Ne rien faire\")\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The linear programming formulation to find the optimal policy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.3",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
