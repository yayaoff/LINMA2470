{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description du MDP et exemple d'utilisation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using POMDPs\n",
    "using QuickPOMDPs\n",
    "using Distributions\n",
    "using Distributions: DiscreteUniform, pdf\n",
    "using POMDPTools: Deterministic\n",
    "using POMDPTools\n",
    "using Statistics\n",
    "using POMDPs: states, actions, transition, reward\n",
    "using TabularTDLearning\n",
    "using Random\n",
    "using StatsBase: Weights\n",
    "using Plots\n",
    "using DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QuickMDP{Base.UUID(\"279547a2-79cc-4c13-8ce8-0dfca7e8c6bd\"), Int64, Int64, @NamedTuple{stateindex::Dict{Int64, Int64}, isterminal::Bool, actionindex::Dict{Int64, Int64}, initialstate::Deterministic{Int64}, states::UnitRange{Int64}, actions::Vector{Int64}, discount::Float64, gen::var\"#11#12\"}}((stateindex = Dict(5 => 6, 16 => 17, 7 => 8, 20 => 21, 12 => 13, 8 => 9, 17 => 18, 1 => 2, 19 => 20, 0 => 1â€¦), isterminal = false, actionindex = Dict(0 => 1, 1 => 2), initialstate = Deterministic{Int64}(10), states = 0:20, actions = [0, 1], discount = 0.99, gen = var\"#11#12\"()))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "const MAX_INVENTORY = 20\n",
    "const MAX_STORE = 10\n",
    "const ORDER_SIZE = 5\n",
    "\n",
    "const holding_cost_store = 2\n",
    "const holding_cost_parking = 4\n",
    "const order_cost = 20\n",
    "const stockout_penalty = 50\n",
    "\n",
    "const demand_dist = DiscreteUniform(0, 10)\n",
    "\n",
    "mdp = QuickMDP(\n",
    "    \n",
    "    states = 0:MAX_INVENTORY,\n",
    "    actions = [0, 1],\n",
    "    discount = 0.99,\n",
    "\n",
    "    gen = function (s, a, rng)\n",
    "        order_qty = a == 1 ? min(ORDER_SIZE, MAX_INVENTORY - s) : 0\n",
    "        new_stock = s + order_qty\n",
    "\n",
    "        d = rand(rng, demand_dist)\n",
    "        sold = min(d, new_stock)\n",
    "        sp = new_stock - sold\n",
    "\n",
    "        lost_sales = max(d - new_stock, 0)\n",
    "\n",
    "        in_store = min(sp, MAX_STORE)\n",
    "        in_parking = max(sp - MAX_STORE, 0)\n",
    "\n",
    "        cost = 0\n",
    "        cost += a == 1 ? order_cost : 0\n",
    "        cost += in_store * holding_cost_store\n",
    "        cost += in_parking * holding_cost_parking\n",
    "        cost += lost_sales * stockout_penalty\n",
    "\n",
    "        r = -cost\n",
    "\n",
    "        return (sp, r, d)\n",
    "    end,\n",
    "    initialstate = Deterministic(10)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From 8 --(a=1)--> 11,  Demand: 2, Reward = -44\n",
      "From 8 --(a=1)--> 4,  Demand: 9, Reward = -28\n",
      "From 8 --(a=1)--> 9,  Demand: 4, Reward = -38\n",
      "From 8 --(a=1)--> 6,  Demand: 7, Reward = -32\n",
      "From 8 --(a=1)--> 6,  Demand: 7, Reward = -32\n",
      "From 8 --(a=1)--> 6,  Demand: 7, Reward = -32\n",
      "From 8 --(a=1)--> 3,  Demand: 10, Reward = -26\n",
      "From 8 --(a=1)--> 5,  Demand: 8, Reward = -30\n",
      "From 8 --(a=1)--> 11,  Demand: 2, Reward = -44\n",
      "From 8 --(a=1)--> 12,  Demand: 1, Reward = -48\n"
     ]
    }
   ],
   "source": [
    "using Random\n",
    "rng = MersenneTwister(42)\n",
    "s = 8\n",
    "a = 1\n",
    "for i in 1:10\n",
    "    sp, r, d = POMDPs.gen(mdp, s, a, rng)\n",
    "    println(\"From $s --(a=$a)--> $sp,  Demand: $d, Reward = $r\")\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `C:\\Users\\mathj\\.julia\\environments\\v1.11\\Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `C:\\Users\\mathj\\.julia\\environments\\v1.11\\Manifest.toml`\n"
     ]
    }
   ],
   "source": [
    "import Pkg; Pkg.add(\"TabularTDLearning\")\n",
    "using POMDPs\n",
    "using QuickPOMDPs\n",
    "using POMDPTools\n",
    "using TabularTDLearning\n",
    "using Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train_q_learning_agent (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function train_q_learning_agent(mdp, alpha::Float64, epsilon::Float64; iterations=10_000)\n",
    "    ql = QLearningSolver(\n",
    "        max_iterations=iterations,\n",
    "        alpha=alpha,\n",
    "        epsilon=epsilon,\n",
    "        gamma=mdp.discount\n",
    "    )\n",
    "    policy = solve(ql, mdp)\n",
    "    return policy, ql.q\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function Îµ_greedy_action(Q, s, actions, Îµ)\n",
    "    if rand() < Îµ\n",
    "        return rand(actions)\n",
    "    else\n",
    "        return argmax(a -> Q[s][a], actions)\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function sarsa_learn(mdp, Î±, Îµ, Î³=0.99; episodes=10_000, max_steps=100)\n",
    "    sta = collect(states(mdp))\n",
    "    ac = collect(actions(mdp))\n",
    "\n",
    "    Q = Dict(s => Dict(a => 0.0 for a in ac) for s in sta)\n",
    "    rng = MersenneTwister(42)\n",
    "\n",
    "    for ep in 1:episodes\n",
    "        s = rand(rng, sta)\n",
    "        a = Îµ_greedy_action(Q, s, ac, Îµ)\n",
    "\n",
    "        for step in 1:max_steps\n",
    "            transitions = transition(mdp, s, a)\n",
    "            probs = [p for (_, p) in transitions]\n",
    "            next_states = [sp for (sp, _) in transitions]\n",
    "            sâ€² = sample(rng, next_states, Weights(probs))\n",
    "\n",
    "            r = reward(mdp, s, a, sâ€²)\n",
    "            aâ€² = Îµ_greedy_action(Q, sâ€², ac, Îµ)\n",
    "\n",
    "            Q[s][a] += Î± * (r + Î³ * Q[sâ€²][aâ€²] - Q[s][a])\n",
    "            s, a = sâ€², aâ€²\n",
    "        end\n",
    "    end\n",
    "\n",
    "    policy = Dict(s => argmax(a -> Q[s][a], ac) for s in sta)\n",
    "    return policy, Q\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy, Q = sarsa_learn(mdp, 0.9, 0.9)\n",
    "\n",
    "println(\"SARSA Policy:\")\n",
    "for st in sort(collect(keys(policy)))\n",
    "    println(\"Inventory $st â†’ Action: \", policy[st] == 1 ? \"Order\" : \"Do Not Order\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function evaluate_policy(mdp, policy; episodes=100, max_steps=100)\n",
    "    total_reward = 0.0\n",
    "    rng = MersenneTwister(123)\n",
    "\n",
    "    for _ in 1:episodes\n",
    "        s = 10  # initial inventory\n",
    "        r_total = 0.0\n",
    "        for _ in 1:max_steps\n",
    "            a = policy[s]\n",
    "            trans = transition(mdp, s, a)\n",
    "            probs = [p for (_, p) in trans]\n",
    "            next_states = [sp for (sp, _) in trans]\n",
    "            sâ€² = sample(rng, next_states, Weights(probs))\n",
    "            r = reward(mdp, s, a, sâ€²)\n",
    "            r_total += r\n",
    "            s = sâ€²\n",
    "        end\n",
    "        total_reward += r_total\n",
    "    end\n",
    "\n",
    "    return total_reward / episodes\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = [0.2, 0.1, 0.01, 1e-5]\n",
    "epsilons = [0.2, 0.1, 0.01, 1e-5]\n",
    "\n",
    "results = []\n",
    "\n",
    "for Î± in alphas, Îµ in epsilons\n",
    "    println(\"Training SARSA with Î± = $Î±, Îµ = $Îµ\")\n",
    "    policy, Q = sarsa_learn(mdp, Î±, Îµ)\n",
    "    avg_reward = evaluate_policy(mdp, policy)\n",
    "    push!(results, (Î±, Îµ, round(avg_reward, digits=2)))\n",
    "end\n",
    "\n",
    "println(\"\\nPerformance Summary (Average Reward per Policy):\")\n",
    "println(\"Î±\\tÏµ\\tAverage Reward\")\n",
    "for (Î±, Îµ, reward) in results\n",
    "    println(\"$Î±\\t$Îµ\\t$reward\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = DataFrame(results, [:alpha, :epsilon, :avg_reward])\n",
    "alphas = sort(unique(df.alpha))\n",
    "epsilons = sort(unique(df.epsilon))\n",
    "\n",
    "# Build Z matrix safely\n",
    "Z = fill(NaN, length(alphas), length(epsilons))\n",
    "for (i, Î±) in enumerate(alphas)\n",
    "    for (j, Ïµ) in enumerate(epsilons)\n",
    "        row = filter(r -> r.alpha == Î± && r.epsilon == Ïµ, eachrow(df))\n",
    "        if !isempty(row)\n",
    "            Z[i, j] = row[1][:avg_reward]\n",
    "        end\n",
    "    end\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap(\n",
    "    epsilons,\n",
    "    alphas,\n",
    "    Z;\n",
    "    xlabel = \"Îµ (Exploration Rate)\",\n",
    "    ylabel = \"Î± (Learning Rate)\",\n",
    "    title = \"SARSA Performance (Average Reward)\",\n",
    "    colorbar_title = \"Reward\",\n",
    "    c = :coolwarm,\n",
    "    clims = (minimum(Z), maximum(Z))  # adjust color scaling\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The value iteration algorithm to find the optimal policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "value_iteration_gen (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function value_iteration_gen(mdp; Î³=POMDPs.discount(mdp), Î¸=1e-3, max_iter=100000, n_samples=100)\n",
    "    states = collect(POMDPs.states(mdp))\n",
    "    actions_per_state = Dict(s => collect(POMDPs.actions(mdp, s)) for s in states)\n",
    "\n",
    "    V = Dict(s => 0.0 for s in states)\n",
    "    Ï€ = Dict(s => first(actions_per_state[s]) for s in states)\n",
    "\n",
    "    rng = Random.MersenneTwister(42)  # fixe pour reproductibilitÃ©\n",
    "\n",
    "    for iter in 1:max_iter\n",
    "        Î” = 0.0\n",
    "        V_new = copy(V)\n",
    "\n",
    "        for s in states\n",
    "            v_old = V[s]\n",
    "            best_value = -Inf\n",
    "            best_action = nothing\n",
    "\n",
    "            for a in actions_per_state[s]\n",
    "                total = 0.0\n",
    "                for _ in 1:n_samples\n",
    "                    sp, r, _ = POMDPs.gen(mdp, s, a, rng)\n",
    "                    total += r + Î³ * V[sp]\n",
    "                end\n",
    "                value = total / n_samples\n",
    "\n",
    "                if value > best_value\n",
    "                    best_value = value\n",
    "                    best_action = a\n",
    "                end\n",
    "            end\n",
    "\n",
    "            V_new[s] = best_value\n",
    "            Ï€[s] = best_action\n",
    "            Î” = max(Î”, abs(v_old - best_value))\n",
    "        end\n",
    "\n",
    "        V = V_new\n",
    "        if Î” < Î¸\n",
    "            println(\"Convergence atteinte (Î” < Î¸ = $Î¸).\")\n",
    "            break\n",
    "        end\n",
    "    end\n",
    "\n",
    "    return V, Ï€\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stock 0 â†’ Action optimale : Commander\n",
      "Stock 1 â†’ Action optimale : Commander\n",
      "Stock 2 â†’ Action optimale : Commander\n",
      "Stock 3 â†’ Action optimale : Commander\n",
      "Stock 4 â†’ Action optimale : Commander\n",
      "Stock 5 â†’ Action optimale : Commander\n",
      "Stock 6 â†’ Action optimale : Commander\n",
      "Stock 7 â†’ Action optimale : Commander\n",
      "Stock 8 â†’ Action optimale : Commander\n",
      "Stock 9 â†’ Action optimale : Commander\n",
      "Stock 10 â†’ Action optimale : Commander\n",
      "Stock 11 â†’ Action optimale : Commander\n",
      "Stock 12 â†’ Action optimale : Commander\n",
      "Stock 13 â†’ Action optimale : Commander\n",
      "Stock 14 â†’ Action optimale : Ne rien faire\n",
      "Stock 15 â†’ Action optimale : Ne rien faire\n",
      "Stock 16 â†’ Action optimale : Ne rien faire\n",
      "Stock 17 â†’ Action optimale : Ne rien faire\n",
      "Stock 18 â†’ Action optimale : Ne rien faire\n",
      "Stock 19 â†’ Action optimale : Ne rien faire\n",
      "Stock 20 â†’ Action optimale : Ne rien faire\n"
     ]
    }
   ],
   "source": [
    "V_opt, Ï€_opt = value_iteration_gen(mdp)\n",
    "for s in sort(collect(keys(Ï€_opt)))\n",
    "    println(\"Stock $s â†’ Action optimale : \", Ï€_opt[s] == 1 ? \"Commander\" : \"Ne rien faire\")\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The linear programming formulation to find the optimal policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "reward (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "using JuMP\n",
    "using GLPK\n",
    "\n",
    "states = 0:MAX_INVENTORY\n",
    "actions = [0, 1]\n",
    "Î³ = discount(mdp)\n",
    "\n",
    "function transition(s, a)\n",
    "    outcomes = []\n",
    "    order_qty = a == 1 ? min(ORDER_SIZE, MAX_INVENTORY - s) : 0\n",
    "    new_stock = s + order_qty\n",
    "\n",
    "    for d in support(demand_dist)\n",
    "        sold = min(d, new_stock)\n",
    "        sp = new_stock - sold\n",
    "        prob = pdf(demand_dist, d)\n",
    "        push!(outcomes, (sp, prob, d))\n",
    "    end\n",
    "    return outcomes\n",
    "end\n",
    "\n",
    "function reward(s, a, sp, d)\n",
    "    order_qty = a == 1 ? min(ORDER_SIZE, MAX_INVENTORY - s) : 0\n",
    "    new_stock = s + order_qty\n",
    "\n",
    "    lost_sales = max(d - new_stock, 0)\n",
    "    in_store = min(sp, MAX_STORE)\n",
    "    in_parking = max(sp - MAX_STORE, 0)\n",
    "\n",
    "    cost = 0\n",
    "    cost += a == 1 ? order_cost : 0\n",
    "    cost += in_store * holding_cost_store\n",
    "    cost += in_parking * holding_cost_parking\n",
    "    cost += lost_sales * stockout_penalty\n",
    "\n",
    "    return -cost\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict{Int64, Float64} with 21 entries:\n",
       "  5  => -4880.4\n",
       "  16 => -4810.99\n",
       "  20 => -4810.05\n",
       "  12 => -4829.17\n",
       "  8  => -4845.59\n",
       "  17 => -4809.17\n",
       "  1  => -4982.53\n",
       "  19 => -4808.83\n",
       "  0  => -5017.85\n",
       "  6  => -4865.76\n",
       "  11 => -4830.99\n",
       "  9  => -4838.93\n",
       "  14 => -4818.93\n",
       "  3  => -4923.22\n",
       "  7  => -4854.32\n",
       "  4  => -4899.66\n",
       "  13 => -4825.59\n",
       "  15 => -4814.15\n",
       "  2  => -4950.9\n",
       "  â‹®  => â‹®"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = Model(GLPK.Optimizer)\n",
    "@variable(model, v[s in states])  # v(s) pour chaque Ã©tat\n",
    "\n",
    "# Contraintes : Bellman optimality\n",
    "for s in states\n",
    "    for a in actions\n",
    "        expected_value = 0.0\n",
    "        for (sp, prob, d) in transition(s, a)\n",
    "            r = reward(s, a, sp, d)\n",
    "            expected_value += prob * (r + Î³ * v[sp])\n",
    "        end\n",
    "        @constraint(model, v[s] â‰¥ expected_value)\n",
    "    end\n",
    "end\n",
    "\n",
    "# Objectif : minimiser somme pondÃ©rÃ©e (Î±(s) = 1 âˆ€s)\n",
    "@objective(model, Min, sum(v[s] for s in states))\n",
    "\n",
    "optimize!(model)\n",
    "\n",
    "# Valeur optimale pour chaque Ã©tat\n",
    "V_lp = Dict(s => JuMP.value(v[s]) for s in states)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Politique optimale obtenue par programmation linÃ©aire :\n",
      "Stock 0 â†’ Commander\n",
      "Stock 1 â†’ Commander\n",
      "Stock 2 â†’ Commander\n",
      "Stock 3 â†’ Commander\n",
      "Stock 4 â†’ Commander\n",
      "Stock 5 â†’ Commander\n",
      "Stock 6 â†’ Commander\n",
      "Stock 7 â†’ Commander\n",
      "Stock 8 â†’ Commander\n",
      "Stock 9 â†’ Commander\n",
      "Stock 10 â†’ Commander\n",
      "Stock 11 â†’ Commander\n",
      "Stock 12 â†’ Commander\n",
      "Stock 13 â†’ Ne rien faire\n",
      "Stock 14 â†’ Ne rien faire\n",
      "Stock 15 â†’ Ne rien faire\n",
      "Stock 16 â†’ Ne rien faire\n",
      "Stock 17 â†’ Ne rien faire\n",
      "Stock 18 â†’ Ne rien faire\n",
      "Stock 19 â†’ Ne rien faire\n",
      "Stock 20 â†’ Ne rien faire\n"
     ]
    }
   ],
   "source": [
    "Ï€_lp = Dict()\n",
    "\n",
    "for s in states\n",
    "    best_value = -Inf\n",
    "    best_action = nothing\n",
    "    for a in actions\n",
    "        total = 0.0\n",
    "        for (sp, prob, d) in transition(s, a)\n",
    "            r = reward(s, a, sp, d)\n",
    "            total += prob * (r + Î³ * V_lp[sp])\n",
    "        end\n",
    "        if total > best_value\n",
    "            best_value = total\n",
    "            best_action = a\n",
    "        end\n",
    "    end\n",
    "    Ï€_lp[s] = best_action\n",
    "end\n",
    "\n",
    "println(\"ðŸ“Š Politique optimale obtenue par programmation linÃ©aire :\")\n",
    "for s in states\n",
    "    println(\"Stock $s â†’ \", Ï€_lp[s] == 1 ? \"Commander\" : \"Ne rien faire\")\n",
    "end\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.1",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
