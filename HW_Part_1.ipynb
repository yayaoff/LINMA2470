{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description du MDP et exemple d'utilisation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QuickMDP{Base.UUID(\"279547a2-79cc-4c13-8ce8-0dfca7e8c6bd\"), Int64, Int64, @NamedTuple{stateindex::Dict{Int64, Int64}, isterminal::Bool, actionindex::Dict{Int64, Int64}, initialstate::Deterministic{Int64}, states::UnitRange{Int64}, actions::Vector{Int64}, discount::Float64, gen::var\"#11#12\"}}((stateindex = Dict(5 => 6, 16 => 17, 7 => 8, 20 => 21, 12 => 13, 8 => 9, 17 => 18, 1 => 2, 19 => 20, 0 => 1â€¦), isterminal = false, actionindex = Dict(0 => 1, 1 => 2), initialstate = Deterministic{Int64}(10), states = 0:20, actions = [0, 1], discount = 0.99, gen = var\"#11#12\"()))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "using POMDPs\n",
    "using QuickPOMDPs\n",
    "using Distributions\n",
    "using Random\n",
    "using POMDPTools: Deterministic\n",
    "\n",
    "const MAX_INVENTORY = 20\n",
    "const MAX_STORE = 10\n",
    "const ORDER_SIZE = 5\n",
    "\n",
    "const holding_cost_store = 2\n",
    "const holding_cost_parking = 4\n",
    "const order_cost = 20\n",
    "const stockout_penalty = 50\n",
    "\n",
    "const demand_dist = DiscreteUniform(0, 10)\n",
    "\n",
    "mdp = QuickMDP(\n",
    "    \n",
    "    states = 0:MAX_INVENTORY,\n",
    "    actions = [0, 1],\n",
    "    discount = 0.99,\n",
    "\n",
    "    gen = function (s, a, rng)\n",
    "        order_qty = a == 1 ? min(ORDER_SIZE, MAX_INVENTORY - s) : 0\n",
    "        new_stock = s + order_qty\n",
    "\n",
    "        d = rand(rng, demand_dist)\n",
    "        sold = min(d, new_stock)\n",
    "        sp = new_stock - sold\n",
    "\n",
    "        lost_sales = max(d - new_stock, 0)\n",
    "\n",
    "        in_store = min(sp, MAX_STORE)\n",
    "        in_parking = max(sp - MAX_STORE, 0)\n",
    "\n",
    "        cost = 0\n",
    "        cost += a == 1 ? order_cost : 0\n",
    "        cost += in_store * holding_cost_store\n",
    "        cost += in_parking * holding_cost_parking\n",
    "        cost += lost_sales * stockout_penalty\n",
    "\n",
    "        r = -cost\n",
    "\n",
    "        return (sp, r, d)\n",
    "    end,\n",
    "    initialstate = Deterministic(10)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From 8 --(a=1)--> 11,  Demand: 2, Reward = -44\n",
      "From 8 --(a=1)--> 4,  Demand: 9, Reward = -28\n",
      "From 8 --(a=1)--> 9,  Demand: 4, Reward = -38\n",
      "From 8 --(a=1)--> 6,  Demand: 7, Reward = -32\n",
      "From 8 --(a=1)--> 6,  Demand: 7, Reward = -32\n",
      "From 8 --(a=1)--> 6,  Demand: 7, Reward = -32\n",
      "From 8 --(a=1)--> 3,  Demand: 10, Reward = -26\n",
      "From 8 --(a=1)--> 5,  Demand: 8, Reward = -30\n",
      "From 8 --(a=1)--> 11,  Demand: 2, Reward = -44\n",
      "From 8 --(a=1)--> 12,  Demand: 1, Reward = -48\n"
     ]
    }
   ],
   "source": [
    "using Random\n",
    "rng = MersenneTwister(42)\n",
    "s = 8\n",
    "a = 1\n",
    "for i in 1:10\n",
    "    sp, r, d = POMDPs.gen(mdp, s, a, rng)\n",
    "    println(\"From $s --(a=$a)--> $sp,  Demand: $d, Reward = $r\")\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `C:\\Users\\mathj\\.julia\\environments\\v1.11\\Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `C:\\Users\\mathj\\.julia\\environments\\v1.11\\Manifest.toml`\n"
     ]
    }
   ],
   "source": [
    "import Pkg; Pkg.add(\"TabularTDLearning\")\n",
    "using POMDPs\n",
    "using QuickPOMDPs\n",
    "using POMDPTools\n",
    "using TabularTDLearning\n",
    "using Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train_q_learning_agent (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function train_q_learning_agent(mdp, alpha::Float64, epsilon::Float64; iterations=10_000)\n",
    "    ql = QLearningSolver(\n",
    "        max_iterations=iterations,\n",
    "        alpha=alpha,\n",
    "        epsilon=epsilon,\n",
    "        gamma=mdp.discount\n",
    "    )\n",
    "    policy = solve(ql, mdp)\n",
    "    return policy, ql.q\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "evaluate_policy (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function evaluate_policy(mdp, policy; num_trials=100, max_steps=100)\n",
    "    sim = RolloutSimulator(max_steps=max_steps)\n",
    "    rewards = [simulate(sim, mdp, policy) for _ in 1:num_trials]\n",
    "    return mean(rewards)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The value iteration algorithm to find the optimal policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "value_iteration_gen (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function value_iteration_gen(mdp; Î³=POMDPs.discount(mdp), Î¸=1e-3, max_iter=100000, n_samples=100)\n",
    "    states = collect(POMDPs.states(mdp))\n",
    "    actions_per_state = Dict(s => collect(POMDPs.actions(mdp, s)) for s in states)\n",
    "\n",
    "    V = Dict(s => 0.0 for s in states)\n",
    "    Ï€ = Dict(s => first(actions_per_state[s]) for s in states)\n",
    "\n",
    "    rng = Random.MersenneTwister(42)  # fixe pour reproductibilitÃ©\n",
    "\n",
    "    for iter in 1:max_iter\n",
    "        Î” = 0.0\n",
    "        V_new = copy(V)\n",
    "\n",
    "        for s in states\n",
    "            v_old = V[s]\n",
    "            best_value = -Inf\n",
    "            best_action = nothing\n",
    "\n",
    "            for a in actions_per_state[s]\n",
    "                total = 0.0\n",
    "                for _ in 1:n_samples\n",
    "                    sp, r, _ = POMDPs.gen(mdp, s, a, rng)\n",
    "                    total += r + Î³ * V[sp]\n",
    "                end\n",
    "                value = total / n_samples\n",
    "\n",
    "                if value > best_value\n",
    "                    best_value = value\n",
    "                    best_action = a\n",
    "                end\n",
    "            end\n",
    "\n",
    "            V_new[s] = best_value\n",
    "            Ï€[s] = best_action\n",
    "            Î” = max(Î”, abs(v_old - best_value))\n",
    "        end\n",
    "\n",
    "        V = V_new\n",
    "        if Î” < Î¸\n",
    "            println(\"Convergence atteinte (Î” < Î¸ = $Î¸).\")\n",
    "            break\n",
    "        end\n",
    "    end\n",
    "\n",
    "    return V, Ï€\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stock 0 â†’ Action optimale : Commander\n",
      "Stock 1 â†’ Action optimale : Commander\n",
      "Stock 2 â†’ Action optimale : Commander\n",
      "Stock 3 â†’ Action optimale : Commander\n",
      "Stock 4 â†’ Action optimale : Commander\n",
      "Stock 5 â†’ Action optimale : Commander\n",
      "Stock 6 â†’ Action optimale : Commander\n",
      "Stock 7 â†’ Action optimale : Commander\n",
      "Stock 8 â†’ Action optimale : Commander\n",
      "Stock 9 â†’ Action optimale : Commander\n",
      "Stock 10 â†’ Action optimale : Commander\n",
      "Stock 11 â†’ Action optimale : Commander\n",
      "Stock 12 â†’ Action optimale : Commander\n",
      "Stock 13 â†’ Action optimale : Commander\n",
      "Stock 14 â†’ Action optimale : Ne rien faire\n",
      "Stock 15 â†’ Action optimale : Ne rien faire\n",
      "Stock 16 â†’ Action optimale : Ne rien faire\n",
      "Stock 17 â†’ Action optimale : Ne rien faire\n",
      "Stock 18 â†’ Action optimale : Ne rien faire\n",
      "Stock 19 â†’ Action optimale : Ne rien faire\n",
      "Stock 20 â†’ Action optimale : Ne rien faire\n"
     ]
    }
   ],
   "source": [
    "V_opt, Ï€_opt = value_iteration_gen(mdp)\n",
    "for s in sort(collect(keys(Ï€_opt)))\n",
    "    println(\"Stock $s â†’ Action optimale : \", Ï€_opt[s] == 1 ? \"Commander\" : \"Ne rien faire\")\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The linear programming formulation to find the optimal policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "reward (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "using JuMP\n",
    "using GLPK\n",
    "\n",
    "states = 0:MAX_INVENTORY\n",
    "actions = [0, 1]\n",
    "Î³ = discount(mdp)\n",
    "\n",
    "function transition(s, a)\n",
    "    outcomes = []\n",
    "    order_qty = a == 1 ? min(ORDER_SIZE, MAX_INVENTORY - s) : 0\n",
    "    new_stock = s + order_qty\n",
    "\n",
    "    for d in support(demand_dist)\n",
    "        sold = min(d, new_stock)\n",
    "        sp = new_stock - sold\n",
    "        prob = pdf(demand_dist, d)\n",
    "        push!(outcomes, (sp, prob, d))\n",
    "    end\n",
    "    return outcomes\n",
    "end\n",
    "\n",
    "function reward(s, a, sp, d)\n",
    "    order_qty = a == 1 ? min(ORDER_SIZE, MAX_INVENTORY - s) : 0\n",
    "    new_stock = s + order_qty\n",
    "\n",
    "    lost_sales = max(d - new_stock, 0)\n",
    "    in_store = min(sp, MAX_STORE)\n",
    "    in_parking = max(sp - MAX_STORE, 0)\n",
    "\n",
    "    cost = 0\n",
    "    cost += a == 1 ? order_cost : 0\n",
    "    cost += in_store * holding_cost_store\n",
    "    cost += in_parking * holding_cost_parking\n",
    "    cost += lost_sales * stockout_penalty\n",
    "\n",
    "    return -cost\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict{Int64, Float64} with 21 entries:\n",
       "  5  => -4880.4\n",
       "  16 => -4810.99\n",
       "  20 => -4810.05\n",
       "  12 => -4829.17\n",
       "  8  => -4845.59\n",
       "  17 => -4809.17\n",
       "  1  => -4982.53\n",
       "  19 => -4808.83\n",
       "  0  => -5017.85\n",
       "  6  => -4865.76\n",
       "  11 => -4830.99\n",
       "  9  => -4838.93\n",
       "  14 => -4818.93\n",
       "  3  => -4923.22\n",
       "  7  => -4854.32\n",
       "  4  => -4899.66\n",
       "  13 => -4825.59\n",
       "  15 => -4814.15\n",
       "  2  => -4950.9\n",
       "  â‹®  => â‹®"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = Model(GLPK.Optimizer)\n",
    "@variable(model, v[s in states])  # v(s) pour chaque Ã©tat\n",
    "\n",
    "# Contraintes : Bellman optimality\n",
    "for s in states\n",
    "    for a in actions\n",
    "        expected_value = 0.0\n",
    "        for (sp, prob, d) in transition(s, a)\n",
    "            r = reward(s, a, sp, d)\n",
    "            expected_value += prob * (r + Î³ * v[sp])\n",
    "        end\n",
    "        @constraint(model, v[s] â‰¥ expected_value)\n",
    "    end\n",
    "end\n",
    "\n",
    "# Objectif : minimiser somme pondÃ©rÃ©e (Î±(s) = 1 âˆ€s)\n",
    "@objective(model, Min, sum(v[s] for s in states))\n",
    "\n",
    "optimize!(model)\n",
    "\n",
    "# Valeur optimale pour chaque Ã©tat\n",
    "V_lp = Dict(s => JuMP.value(v[s]) for s in states)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Politique optimale obtenue par programmation linÃ©aire :\n",
      "Stock 0 â†’ Commander\n",
      "Stock 1 â†’ Commander\n",
      "Stock 2 â†’ Commander\n",
      "Stock 3 â†’ Commander\n",
      "Stock 4 â†’ Commander\n",
      "Stock 5 â†’ Commander\n",
      "Stock 6 â†’ Commander\n",
      "Stock 7 â†’ Commander\n",
      "Stock 8 â†’ Commander\n",
      "Stock 9 â†’ Commander\n",
      "Stock 10 â†’ Commander\n",
      "Stock 11 â†’ Commander\n",
      "Stock 12 â†’ Commander\n",
      "Stock 13 â†’ Ne rien faire\n",
      "Stock 14 â†’ Ne rien faire\n",
      "Stock 15 â†’ Ne rien faire\n",
      "Stock 16 â†’ Ne rien faire\n",
      "Stock 17 â†’ Ne rien faire\n",
      "Stock 18 â†’ Ne rien faire\n",
      "Stock 19 â†’ Ne rien faire\n",
      "Stock 20 â†’ Ne rien faire\n"
     ]
    }
   ],
   "source": [
    "Ï€_lp = Dict()\n",
    "\n",
    "for s in states\n",
    "    best_value = -Inf\n",
    "    best_action = nothing\n",
    "    for a in actions\n",
    "        total = 0.0\n",
    "        for (sp, prob, d) in transition(s, a)\n",
    "            r = reward(s, a, sp, d)\n",
    "            total += prob * (r + Î³ * V_lp[sp])\n",
    "        end\n",
    "        if total > best_value\n",
    "            best_value = total\n",
    "            best_action = a\n",
    "        end\n",
    "    end\n",
    "    Ï€_lp[s] = best_action\n",
    "end\n",
    "\n",
    "println(\"ðŸ“Š Politique optimale obtenue par programmation linÃ©aire :\")\n",
    "for s in states\n",
    "    println(\"Stock $s â†’ \", Ï€_lp[s] == 1 ? \"Commander\" : \"Ne rien faire\")\n",
    "end\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.1",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
